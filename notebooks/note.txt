doc file: https://docs.google.com/document/d/19X8nYpwktiGCqg1jukvy2Xc8JaxtA6LoP83zBLRcg2E/edit?tab=t.0



Linear Regression

z=w1​x1​+w2​x2​+⋯+b

we train, on different sets of weights, and bs, until we get least error, that is linear Regression. We get values from -infinity to infinity


example: price = 50_000 * bedrooms + 20_000 * bathrooms + 10_000


Logistic Regression

basically same as linear Regression, but uses a signoid function
p=σ(z)=1/(1+ exp(-z))

p=P(y=1∣x)
log((1−p)/p​)=w1​x1​+⋯+b

utilizes log loss


Overfitting
THIS exact dog with this exact spot pattern is a dog
Underfitting
ALL animals are dogs

Bias–Variance Tradeoff

Bias = how wrong the model is by design, High bias: Underfitting
Variance = how sensitive the model is to data,  High variance: Overfitting

Managing Bias and Variance, is what our intent is for our purpose.



Regularization
Training minimizes loss, regulation adds penalty, (basically imagine a knob, changes weight by how much each time)
Loss+λ⋅Penalty(weights)

L2 Regularization(Ridge)
Penalty:   ∑w2

L1 Regularization(lasso)
Penalty:  ∑∣w∣



XGBoost = Gradient Boosted Decision Trees
LogLoss=−[ylog(p)+(1−y)log(1−p)]



Multiplayer Perception:
Neural Network, you sent outputs to another neuron and get better results, each iteration is called an Epoch(s)
Guess, change , repeat
Output, Back propagation, Epoch(s)

Activation funtion:
WHen data is complicated, it might not be easy to use linear funtion, we might use other functions



Back Propagation:
https://www.geeksforgeeks.org/machine-learning/backpropagation-in-neural-network/

its iteratively changing the weights and biases for all the neurons, then averages over the entire training nudges the weights and biases,
https://youtu.be/tIeHLnjs5U8?si=v0w7OUI1ymnn3bp6


to learn pycharm/neural network
https://www.kaggle.com/code/edomingo/intro-to-pytorch-in-10-code-cells/notebook


Stochastic Gradient Descent
Its basically shuffled data, take a piece of pie at a time approach
