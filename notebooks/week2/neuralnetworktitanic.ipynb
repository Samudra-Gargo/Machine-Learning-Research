{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "c68d5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df = pd.read_csv(\"../../data/processed/titanic3_eda.csv\")\n",
    "\n",
    "target = \"survived\"\n",
    "features = [\"pclass\", \"sex\", \"age\", \"fare\", \"sibsp\", \"parch\", \"embarked\", \"is_child\"]\n",
    "\n",
    "X = df[features].copy()\n",
    "y = df[target].astype(np.float32).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f23212da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: (1047, 8) Test: (262, 8)\n",
      "Train survival rate: 0.38204393 Test survival rate: 0.3816794\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42,\n",
    "    stratify=y\n",
    ")\n",
    "\n",
    "print(\"Train:\", X_train.shape, \"Test:\", X_test.shape)\n",
    "print(\"Train survival rate:\", y_train.mean(), \"Test survival rate:\", y_test.mean())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3204fe70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After preprocessing: (1047, 11) (262, 11)\n"
     ]
    }
   ],
   "source": [
    "num_cols = [\"pclass\", \"age\", \"fare\", \"sibsp\", \"parch\", \"is_child\"]\n",
    "cat_cols = [\"sex\", \"embarked\"]\n",
    "\n",
    "preprocess = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# Fit on TRAIN ONLY, then transform train/test\n",
    "X_train_np = preprocess.fit_transform(X_train)\n",
    "X_test_np  = preprocess.transform(X_test)\n",
    "\n",
    "# Convert to dense if needed (sometimes it's sparse)\n",
    "X_train_np = X_train_np.toarray() if hasattr(X_train_np, \"toarray\") else X_train_np\n",
    "X_test_np  = X_test_np.toarray() if hasattr(X_test_np, \"toarray\") else X_test_np\n",
    "\n",
    "print(\"After preprocessing:\", X_train_np.shape, X_test_np.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6e9ef2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_t = torch.tensor(X_train_np, dtype=torch.float32)\n",
    "X_test_t  = torch.tensor(X_test_np, dtype=torch.float32)\n",
    "\n",
    "y_train_t = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
    "y_test_t  = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
    "\n",
    "train_ds = TensorDataset(X_train_t, y_train_t)\n",
    "test_ds  = TensorDataset(X_test_t, y_test_t)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=256, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "513a3609",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (net): Sequential(\n",
       "    (0): Linear(in_features=11, out_features=32, bias=True)\n",
       "    (1): ReLU()\n",
       "    (2): Linear(in_features=32, out_features=1, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(in_dim, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1)   # logits (raw scores), NOT probabilities\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model = MLP(in_dim=X_train_t.shape[1])\n",
    "model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "32138829",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 00 | train_loss=0.6543 train_acc=0.6425 | test_loss=0.6413 test_acc=0.6794\n",
      "Epoch 01 | train_loss=0.6308 train_acc=0.6793 | test_loss=0.6179 test_acc=0.7023\n",
      "Epoch 02 | train_loss=0.6115 train_acc=0.7058 | test_loss=0.5956 test_acc=0.7061\n",
      "Epoch 03 | train_loss=0.5903 train_acc=0.7190 | test_loss=0.5751 test_acc=0.7137\n",
      "Epoch 04 | train_loss=0.5673 train_acc=0.7315 | test_loss=0.5566 test_acc=0.7099\n",
      "Epoch 05 | train_loss=0.5512 train_acc=0.7440 | test_loss=0.5397 test_acc=0.7214\n",
      "Epoch 06 | train_loss=0.5400 train_acc=0.7534 | test_loss=0.5249 test_acc=0.7328\n",
      "Epoch 07 | train_loss=0.5254 train_acc=0.7547 | test_loss=0.5118 test_acc=0.7405\n",
      "Epoch 08 | train_loss=0.5104 train_acc=0.7611 | test_loss=0.5000 test_acc=0.7519\n",
      "Epoch 09 | train_loss=0.5040 train_acc=0.7650 | test_loss=0.4890 test_acc=0.7786\n",
      "Epoch 10 | train_loss=0.4933 train_acc=0.7756 | test_loss=0.4784 test_acc=0.7901\n",
      "Epoch 11 | train_loss=0.4871 train_acc=0.7811 | test_loss=0.4709 test_acc=0.7939\n",
      "Epoch 12 | train_loss=0.4771 train_acc=0.7826 | test_loss=0.4637 test_acc=0.7939\n",
      "Epoch 13 | train_loss=0.4726 train_acc=0.7813 | test_loss=0.4560 test_acc=0.7901\n",
      "Epoch 14 | train_loss=0.4698 train_acc=0.7795 | test_loss=0.4499 test_acc=0.8053\n",
      "Epoch 15 | train_loss=0.4738 train_acc=0.7748 | test_loss=0.4451 test_acc=0.8282\n",
      "Epoch 16 | train_loss=0.4610 train_acc=0.7852 | test_loss=0.4416 test_acc=0.8244\n",
      "Epoch 17 | train_loss=0.4528 train_acc=0.7901 | test_loss=0.4379 test_acc=0.8321\n",
      "Epoch 18 | train_loss=0.4576 train_acc=0.7889 | test_loss=0.4341 test_acc=0.8282\n",
      "Epoch 19 | train_loss=0.4557 train_acc=0.7907 | test_loss=0.4317 test_acc=0.8282\n",
      "Epoch 20 | train_loss=0.4550 train_acc=0.7900 | test_loss=0.4302 test_acc=0.8206\n",
      "Epoch 21 | train_loss=0.4572 train_acc=0.7880 | test_loss=0.4272 test_acc=0.8359\n",
      "Epoch 22 | train_loss=0.4508 train_acc=0.7880 | test_loss=0.4264 test_acc=0.8321\n",
      "Epoch 23 | train_loss=0.4438 train_acc=0.7947 | test_loss=0.4251 test_acc=0.8321\n",
      "Epoch 24 | train_loss=0.4608 train_acc=0.7893 | test_loss=0.4240 test_acc=0.8321\n",
      "Epoch 25 | train_loss=0.4523 train_acc=0.7888 | test_loss=0.4228 test_acc=0.8321\n",
      "Epoch 26 | train_loss=0.4473 train_acc=0.7972 | test_loss=0.4215 test_acc=0.8359\n",
      "Epoch 27 | train_loss=0.4475 train_acc=0.7970 | test_loss=0.4202 test_acc=0.8359\n",
      "Epoch 28 | train_loss=0.4358 train_acc=0.7984 | test_loss=0.4195 test_acc=0.8359\n",
      "Epoch 29 | train_loss=0.4368 train_acc=0.8021 | test_loss=0.4188 test_acc=0.8359\n",
      "Epoch 30 | train_loss=0.4370 train_acc=0.8039 | test_loss=0.4180 test_acc=0.8321\n",
      "Epoch 31 | train_loss=0.4360 train_acc=0.8032 | test_loss=0.4162 test_acc=0.8321\n",
      "Epoch 32 | train_loss=0.4413 train_acc=0.7983 | test_loss=0.4167 test_acc=0.8321\n",
      "Epoch 33 | train_loss=0.4350 train_acc=0.8065 | test_loss=0.4158 test_acc=0.8321\n",
      "Epoch 34 | train_loss=0.4377 train_acc=0.8008 | test_loss=0.4152 test_acc=0.8282\n",
      "Epoch 35 | train_loss=0.4386 train_acc=0.8032 | test_loss=0.4143 test_acc=0.8321\n",
      "Epoch 36 | train_loss=0.4380 train_acc=0.7974 | test_loss=0.4142 test_acc=0.8321\n",
      "Epoch 37 | train_loss=0.4343 train_acc=0.8050 | test_loss=0.4148 test_acc=0.8282\n",
      "Epoch 38 | train_loss=0.4383 train_acc=0.8010 | test_loss=0.4130 test_acc=0.8435\n",
      "Epoch 39 | train_loss=0.4350 train_acc=0.8016 | test_loss=0.4124 test_acc=0.8435\n",
      "Epoch 40 | train_loss=0.4352 train_acc=0.8036 | test_loss=0.4125 test_acc=0.8435\n",
      "Epoch 41 | train_loss=0.4272 train_acc=0.8101 | test_loss=0.4135 test_acc=0.8359\n",
      "Epoch 42 | train_loss=0.4358 train_acc=0.8063 | test_loss=0.4121 test_acc=0.8473\n",
      "Epoch 43 | train_loss=0.4316 train_acc=0.7999 | test_loss=0.4130 test_acc=0.8321\n",
      "Epoch 44 | train_loss=0.4335 train_acc=0.8016 | test_loss=0.4132 test_acc=0.8397\n",
      "Epoch 45 | train_loss=0.4280 train_acc=0.8076 | test_loss=0.4116 test_acc=0.8473\n",
      "Epoch 46 | train_loss=0.4265 train_acc=0.8050 | test_loss=0.4117 test_acc=0.8397\n",
      "Epoch 47 | train_loss=0.4304 train_acc=0.8008 | test_loss=0.4111 test_acc=0.8435\n",
      "Epoch 48 | train_loss=0.4384 train_acc=0.7994 | test_loss=0.4108 test_acc=0.8435\n",
      "Epoch 49 | train_loss=0.4277 train_acc=0.8043 | test_loss=0.4118 test_acc=0.8397\n",
      "Epoch 50 | train_loss=0.4214 train_acc=0.8092 | test_loss=0.4109 test_acc=0.8435\n",
      "Epoch 51 | train_loss=0.4222 train_acc=0.8111 | test_loss=0.4106 test_acc=0.8435\n",
      "Epoch 52 | train_loss=0.4217 train_acc=0.8122 | test_loss=0.4109 test_acc=0.8435\n",
      "Epoch 53 | train_loss=0.4257 train_acc=0.8149 | test_loss=0.4100 test_acc=0.8511\n",
      "Epoch 54 | train_loss=0.4296 train_acc=0.8105 | test_loss=0.4099 test_acc=0.8511\n",
      "Epoch 55 | train_loss=0.4229 train_acc=0.8138 | test_loss=0.4105 test_acc=0.8550\n",
      "Epoch 56 | train_loss=0.4224 train_acc=0.8122 | test_loss=0.4103 test_acc=0.8511\n",
      "Epoch 57 | train_loss=0.4196 train_acc=0.8145 | test_loss=0.4098 test_acc=0.8511\n",
      "Epoch 58 | train_loss=0.4282 train_acc=0.8071 | test_loss=0.4099 test_acc=0.8473\n",
      "Epoch 59 | train_loss=0.4251 train_acc=0.8124 | test_loss=0.4096 test_acc=0.8511\n",
      "Epoch 60 | train_loss=0.4270 train_acc=0.8065 | test_loss=0.4089 test_acc=0.8550\n",
      "Epoch 61 | train_loss=0.4281 train_acc=0.8073 | test_loss=0.4090 test_acc=0.8511\n",
      "Epoch 62 | train_loss=0.4263 train_acc=0.8063 | test_loss=0.4085 test_acc=0.8511\n",
      "Epoch 63 | train_loss=0.4191 train_acc=0.8131 | test_loss=0.4078 test_acc=0.8473\n",
      "Epoch 64 | train_loss=0.4184 train_acc=0.8120 | test_loss=0.4083 test_acc=0.8473\n",
      "Epoch 65 | train_loss=0.4195 train_acc=0.8115 | test_loss=0.4090 test_acc=0.8588\n",
      "Epoch 66 | train_loss=0.4237 train_acc=0.8091 | test_loss=0.4078 test_acc=0.8511\n",
      "Epoch 67 | train_loss=0.4170 train_acc=0.8149 | test_loss=0.4091 test_acc=0.8473\n",
      "Epoch 68 | train_loss=0.4198 train_acc=0.8089 | test_loss=0.4077 test_acc=0.8511\n",
      "Epoch 69 | train_loss=0.4190 train_acc=0.8149 | test_loss=0.4077 test_acc=0.8473\n",
      "Epoch 70 | train_loss=0.4300 train_acc=0.8067 | test_loss=0.4072 test_acc=0.8473\n",
      "Epoch 71 | train_loss=0.4186 train_acc=0.8149 | test_loss=0.4095 test_acc=0.8473\n",
      "Epoch 72 | train_loss=0.4173 train_acc=0.8147 | test_loss=0.4087 test_acc=0.8473\n",
      "Epoch 73 | train_loss=0.4180 train_acc=0.8131 | test_loss=0.4076 test_acc=0.8473\n",
      "Epoch 74 | train_loss=0.4176 train_acc=0.8071 | test_loss=0.4066 test_acc=0.8511\n",
      "Epoch 75 | train_loss=0.4150 train_acc=0.8140 | test_loss=0.4065 test_acc=0.8511\n",
      "Epoch 76 | train_loss=0.4206 train_acc=0.8087 | test_loss=0.4072 test_acc=0.8511\n",
      "Epoch 77 | train_loss=0.4152 train_acc=0.8085 | test_loss=0.4069 test_acc=0.8511\n",
      "Epoch 78 | train_loss=0.4218 train_acc=0.8061 | test_loss=0.4068 test_acc=0.8511\n",
      "Epoch 79 | train_loss=0.4239 train_acc=0.8038 | test_loss=0.4066 test_acc=0.8511\n",
      "Epoch 80 | train_loss=0.4230 train_acc=0.8031 | test_loss=0.4071 test_acc=0.8511\n",
      "Epoch 81 | train_loss=0.4203 train_acc=0.8149 | test_loss=0.4071 test_acc=0.8511\n",
      "Epoch 82 | train_loss=0.4264 train_acc=0.8073 | test_loss=0.4065 test_acc=0.8511\n",
      "Epoch 83 | train_loss=0.4269 train_acc=0.8040 | test_loss=0.4079 test_acc=0.8473\n",
      "Epoch 84 | train_loss=0.4180 train_acc=0.8063 | test_loss=0.4067 test_acc=0.8511\n",
      "Epoch 85 | train_loss=0.4233 train_acc=0.8063 | test_loss=0.4073 test_acc=0.8511\n",
      "Epoch 86 | train_loss=0.4214 train_acc=0.8100 | test_loss=0.4057 test_acc=0.8550\n",
      "Epoch 87 | train_loss=0.4200 train_acc=0.8115 | test_loss=0.4070 test_acc=0.8511\n",
      "Epoch 88 | train_loss=0.4146 train_acc=0.8140 | test_loss=0.4062 test_acc=0.8511\n",
      "Epoch 89 | train_loss=0.4261 train_acc=0.8012 | test_loss=0.4050 test_acc=0.8550\n",
      "Epoch 90 | train_loss=0.4195 train_acc=0.8149 | test_loss=0.4073 test_acc=0.8473\n",
      "Epoch 91 | train_loss=0.4197 train_acc=0.8082 | test_loss=0.4067 test_acc=0.8511\n",
      "Epoch 92 | train_loss=0.4195 train_acc=0.8142 | test_loss=0.4053 test_acc=0.8473\n",
      "Epoch 93 | train_loss=0.4172 train_acc=0.8142 | test_loss=0.4066 test_acc=0.8473\n",
      "Epoch 94 | train_loss=0.4079 train_acc=0.8191 | test_loss=0.4063 test_acc=0.8550\n",
      "Epoch 95 | train_loss=0.4209 train_acc=0.8126 | test_loss=0.4056 test_acc=0.8550\n",
      "Epoch 96 | train_loss=0.4147 train_acc=0.8105 | test_loss=0.4054 test_acc=0.8511\n",
      "Epoch 97 | train_loss=0.4199 train_acc=0.8065 | test_loss=0.4067 test_acc=0.8473\n",
      "Epoch 98 | train_loss=0.4119 train_acc=0.8115 | test_loss=0.4075 test_acc=0.8473\n",
      "Epoch 99 | train_loss=0.4171 train_acc=0.8107 | test_loss=0.4062 test_acc=0.8473\n"
     ]
    }
   ],
   "source": [
    "loss_fn = nn.BCEWithLogitsLoss()   # combines sigmoid + BCE in a stable way\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "def accuracy_from_logits(logits, y_true):\n",
    "    probs = torch.sigmoid(logits)\n",
    "    preds = (probs >= 0.5).float()\n",
    "    return (preds == y_true).float().mean().item()\n",
    "\n",
    "for epoch in range(0, 100):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    train_accs = []\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        logits = model(xb)\n",
    "        loss = loss_fn(logits, yb)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        train_losses.append(loss.item())\n",
    "        train_accs.append(accuracy_from_logits(logits.detach(), yb))\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        test_logits = model(X_test_t)\n",
    "        test_loss = loss_fn(test_logits, y_test_t).item()\n",
    "        test_acc = accuracy_from_logits(test_logits, y_test_t)\n",
    "\n",
    "    print(\n",
    "        f\"Epoch {epoch:02d} | \"\n",
    "        f\"train_loss={np.mean(train_losses):.4f} train_acc={np.mean(train_accs):.4f} | \"\n",
    "        f\"test_loss={test_loss:.4f} test_acc={test_acc:.4f}\"\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.14.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
